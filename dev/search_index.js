var documenterSearchIndex = {"docs":
[{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/#NLPModelsIpopt.IpoptSolver","page":"Reference","title":"NLPModelsIpopt.IpoptSolver","text":"IpoptSolver(nlp; kwargs...,)\n\nReturns an IpoptSolver structure to solve the problem nlp with ipopt.\n\n\n\n\n\n","category":"type"},{"location":"reference/#LinearOperators.reset!-Tuple{IpoptSolver, NLPModels.AbstractNLPModel}","page":"Reference","title":"LinearOperators.reset!","text":"solver = reset!(solver::IpoptSolver, nlp::AbstractNLPModel)\n\nReset the solver with the new model nlp.\n\nIf nlp has different bounds on the variables/constraints or a different number of nonzeros elements in the Jacobian/Hessian, then you need to create a new IpoptSolver.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsIpopt.ipopt-Tuple{NLPModels.AbstractNLPModel}","page":"Reference","title":"NLPModelsIpopt.ipopt","text":"output = ipopt(nlp; kwargs...)\n\nSolves the NLPModel problem nlp using Ipopt.\n\nFor advanced usage, first define a IpoptSolver to preallocate the memory used in the algorithm, and then call solve!:     solver = IpoptSolver(nlp)     solve!(solver, nlp; kwargs...)     solve!(solver, nlp, stats; kwargs...)\n\nOptional keyword arguments\n\nx0: a vector of size nlp.meta.nvar to specify an initial primal guess\ny0: a vector of size nlp.meta.ncon to specify an initial dual guess for the general constraints\nzL: a vector of size nlp.meta.nvar to specify initial multipliers for the lower bound constraints\nzU: a vector of size nlp.meta.nvar to specify initial multipliers for the upper bound constraints\n\nAll other keyword arguments will be passed to Ipopt as an option. See https://coin-or.github.io/Ipopt/OPTIONS.html for the list of options accepted.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nExamples\n\nusing NLPModelsIpopt, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nstats = ipopt(nlp, print_level = 0)\n\nusing NLPModelsIpopt, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nsolver = IpoptSolver(nlp);\nstats = solve!(solver, nlp, print_level = 0)\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsIpopt.ipopt-Tuple{NLPModelsModifiers.FeasibilityFormNLS}","page":"Reference","title":"NLPModelsIpopt.ipopt","text":"ipopt(nls::AbstractNLSModel; kwargs...)\n\nSolve the least-squares problem nls using IPOPT by moving the nonlinear residual to the constraints.\n\nArguments\n\nnls::AbstractNLSModel: The least-squares problem to solve.\n\nFor advanced usage, first define an IpoptSolver to preallocate the memory used in the algorithm, and then call solve!:     solver = IpoptSolver(nls)     solve!(solver, nls; kwargs...)\n\nExamples\n\nusing NLPModelsIpopt, ADNLPModels\nnls = ADNLSModel(x -> [x[1] - 1, x[2] - 2], [0.0, 0.0], 2)\nstats = ipopt(nls, print_level = 0)\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsIpopt.set_callbacks-Tuple{NLPModels.AbstractNLPModel}","page":"Reference","title":"NLPModelsIpopt.set_callbacks","text":"set_callbacks(nlp::AbstractNLPModel)\n\nReturn the set of functions needed to instantiate an IpoptProblem.\n\n\n\n\n\n","category":"method"},{"location":"#NLPModelsIpopt.jl-documentation","page":"Home","title":"NLPModelsIpopt.jl documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides a thin IPOPT wrapper for NLPModels, using JuliaOpt/Ipopt.jl internal structures directly.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Please refer to the NLPModels documentation for the API of NLPModels, if needed.","category":"page"},{"location":"#Install","page":"Home","title":"Install","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Install NLPModelsIpopt.jl with the following commands.","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add NLPModelsIpopt","category":"page"},{"location":"#Bug-reports-and-discussions","page":"Home","title":"Bug reports and discussions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want to ask a question not suited for a bug report, feel free to start a discussion here. This forum is for general discussion about this repository and the JuliaSmoothOptimizers, so questions about any of our packages are welcome.","category":"page"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"tips/#Linear-solvers","page":"Performance tips","title":"Linear solvers","text":"","category":"section"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"To improve performance, Ipopt supports multiple linear solvers, the default one is MUMPS but you can modify the linear solver with the keyword argument linear_solver.","category":"page"},{"location":"tips/#HSL","page":"Performance tips","title":"HSL","text":"","category":"section"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"Obtain a license and download HSL_jll.jl from https://licences.stfc.ac.uk/products/Software/HSL/LibHSL.","category":"page"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"There are two versions available: LBT and OpenBLAS. LBT is the recommended option for Julia ≥ v1.9.","category":"page"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"Install this download into your current environment using:","category":"page"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"import Pkg\nPkg.develop(path = \"/full/path/to/HSL_jll.jl\")","category":"page"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"We provide an example with the linear solvers MA27 and MA57:","category":"page"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"using HSL_jll, NLPModelsIpopt\nstats_ma27 = ipopt(nlp, linear_solver=\"ma27\")\nstats_ma57 = ipopt(nlp, linear_solver=\"ma57\")","category":"page"},{"location":"tips/#SPRAL","page":"Performance tips","title":"SPRAL","text":"","category":"section"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"If you use NLPModelsIpopt.jl with Julia ≥ v1.9, the linear solver SPRAL is available. You can use it by setting the linear_solver attribute:","category":"page"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"using NLPModelsIpopt\nstats_spral = ipopt(nlp, linear_solver=\"spral\")","category":"page"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"Note that the following environment variables must be set before starting Julia:","category":"page"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"export OMP_CANCELLATION=TRUE\nexport OMP_PROC_BIND=TRUE","category":"page"},{"location":"tips/#BLAS-and-LAPACK","page":"Performance tips","title":"BLAS and LAPACK","text":"","category":"section"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"With Julia v1.9 or later, Ipopt and the linear solvers MUMPS (default), SPRAL, and HSL are compiled with libblastrampoline (LBT), a library that can change between BLAS and LAPACK backends at runtime.","category":"page"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"The default BLAS and LAPACK backend is OpenBLAS.","category":"page"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"Using LBT, we can also switch dynamically to other BLAS backends such as Intel MKL and Apple Accelerate. Because Ipopt and the linear solvers heavily rely on BLAS and LAPACK routines, using an optimized backend for a particular platform can improve the performance.","category":"page"},{"location":"tips/#MKL","page":"Performance tips","title":"MKL","text":"","category":"section"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"If you have MKL.jl installed, switch to MKL by adding using MKL to your code:","category":"page"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"using MKL  # Replace OpenBLAS by Intel MKL\nusing NLPModelsIpopt","category":"page"},{"location":"tips/#AppleAccelerate","page":"Performance tips","title":"AppleAccelerate","text":"","category":"section"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"If you are using macOS ≥ v13.4 and you have AppleAccelerate.jl installed, you can replace OpenBLAS as follows:","category":"page"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"using AppleAccelerate  # Replace OpenBLAS by Apple Accelerate\nusing NLPModelsIpopt","category":"page"},{"location":"tips/#Display-backends","page":"Performance tips","title":"Display backends","text":"","category":"section"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"Check what backends are loaded using:","category":"page"},{"location":"tips/","page":"Performance tips","title":"Performance tips","text":"import LinearAlgebra\nLinearAlgebra.BLAS.lbt_get_config()","category":"page"},{"location":"tutorial/#Example:-Using-L-BFGS-(limited-memory)-Hessian-approximation-with-Ipopt","page":"Tutorial","title":"Example: Using L-BFGS (limited-memory) Hessian approximation with Ipopt","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"You can call Ipopt with the L-BFGS Hessian approximation by passing the following options:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"stats_ipopt = ipopt(nlp,\n  hessian_approximation=\"limited-memory\",\n  limited_memory_update_type=\"bfgs\",\n  limited_memory_max_history=10)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This will use the L-BFGS method for Hessian approximation with a history size of 10.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Reference:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Ipopt.jl Manual: Hessian approximation","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"NLPModelsIpopt is a thin IPOPT wrapper for NLPModels. In this tutorial we show examples of problems created with NLPModels and solved with Ipopt.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Pages = [\"tutorial.md\"]","category":"page"},{"location":"tutorial/#Simple-problems","page":"Tutorial","title":"Simple problems","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Calling Ipopt is simple:","category":"page"},{"location":"tutorial/#NLPModelsIpopt.ipopt","page":"Tutorial","title":"NLPModelsIpopt.ipopt","text":"output = ipopt(nlp; kwargs...)\n\nSolves the NLPModel problem nlp using Ipopt.\n\nFor advanced usage, first define a IpoptSolver to preallocate the memory used in the algorithm, and then call solve!:     solver = IpoptSolver(nlp)     solve!(solver, nlp; kwargs...)     solve!(solver, nlp, stats; kwargs...)\n\nOptional keyword arguments\n\nx0: a vector of size nlp.meta.nvar to specify an initial primal guess\ny0: a vector of size nlp.meta.ncon to specify an initial dual guess for the general constraints\nzL: a vector of size nlp.meta.nvar to specify initial multipliers for the lower bound constraints\nzU: a vector of size nlp.meta.nvar to specify initial multipliers for the upper bound constraints\n\nAll other keyword arguments will be passed to Ipopt as an option. See https://coin-or.github.io/Ipopt/OPTIONS.html for the list of options accepted.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nExamples\n\nusing NLPModelsIpopt, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nstats = ipopt(nlp, print_level = 0)\n\nusing NLPModelsIpopt, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nsolver = IpoptSolver(nlp);\nstats = solve!(solver, nlp, print_level = 0)\n\n\n\n\n\nipopt(nls::AbstractNLSModel; kwargs...)\n\nSolve the least-squares problem nls using IPOPT by moving the nonlinear residual to the constraints.\n\nArguments\n\nnls::AbstractNLSModel: The least-squares problem to solve.\n\nFor advanced usage, first define an IpoptSolver to preallocate the memory used in the algorithm, and then call solve!:     solver = IpoptSolver(nls)     solve!(solver, nls; kwargs...)\n\nExamples\n\nusing NLPModelsIpopt, ADNLPModels\nnls = ADNLSModel(x -> [x[1] - 1, x[2] - 2], [0.0, 0.0], 2)\nstats = ipopt(nls, print_level = 0)\n\n\n\n\n\n","category":"function"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's create an NLPModel for the Rosenbrock function","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"f(x) = (x_1 - 1)^2 + 100 (x_2 - x_1^2)^2","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"and solve it with Ipopt:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ADNLPModels, NLPModels, NLPModelsIpopt\n\nnlp = ADNLPModel(x -> (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2, [-1.2; 1.0])\nstats = ipopt(nlp)\nprint(stats)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For comparison, we present the same problem and output using JuMP:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using JuMP, Ipopt\n\nmodel = Model(Ipopt.Optimizer)\nx0 = [-1.2; 1.0]\n@variable(model, x[i=1:2], start=x0[i])\n@NLobjective(model, Min, (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2)\noptimize!(model)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here is an example with a constrained problem:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"n = 10\nx0 = ones(n)\nx0[1:2:end] .= -1.2\nlcon = ucon = zeros(n-2)\nnlp = ADNLPModel(x -> sum((x[i] - 1)^2 + 100 * (x[i+1] - x[i]^2)^2 for i = 1:n-1), x0,\n                 x -> [3 * x[k+1]^3 + 2 * x[k+2] - 5 + sin(x[k+1] - x[k+2]) * sin(x[k+1] + x[k+2]) +\n                       4 * x[k+1] - x[k] * exp(x[k] - x[k+1]) - 3 for k = 1:n-2],\n                 lcon, ucon)\nstats = ipopt(nlp, print_level=0)\nprint(stats)","category":"page"},{"location":"tutorial/#Return-value","page":"Tutorial","title":"Return value","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The return value of ipopt is a GenericExecutionStats instance from SolverCore. It contains basic information on the solution returned by the solver. In addition to the built-in fields of GenericExecutionStats, we store the detailed Ipopt output message inside solver_specific[:internal_msg].","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here is an example using the constrained problem solve:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"stats.solver_specific[:internal_msg]","category":"page"},{"location":"tutorial/#Monitoring-optimization-with-callbacks","page":"Tutorial","title":"Monitoring optimization with callbacks","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"You can monitor the optimization process using a callback function. The callback allows you to access the current iterate and constraint violations at each iteration, which is useful for custom stopping criteria, logging, or real-time analysis.","category":"page"},{"location":"tutorial/#Callback-parameters","page":"Tutorial","title":"Callback parameters","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The callback function receives the following parameters from Ipopt:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"alg_mod: algorithm mode (0 = regular, 1 = restoration phase)\niter_count: current iteration number\nobj_value: current objective function value\ninf_pr: primal infeasibility (constraint violation)\ninf_du: dual infeasibility \nmu: complementarity measure\nd_norm: norm of the primal step\nregularization_size: size of regularization\nalpha_du: step size for dual variables\nalpha_pr: step size for primal variables  \nls_trials: number of line search trials","category":"page"},{"location":"tutorial/#Example-usage","page":"Tutorial","title":"Example usage","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here's a complete example showing how to use callbacks to monitor the optimization:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ADNLPModels, NLPModelsIpopt\n\nfunction my_callback(alg_mod, iter_count, obj_value, inf_pr, inf_du, mu, d_norm, regularization_size, alpha_du, alpha_pr, ls_trials, args...)\n    # Log iteration information (these are the standard parameters passed by Ipopt)\n    println(\"Iteration $iter_count:\")\n    println(\"  Objective value = \", obj_value)\n    println(\"  Primal infeasibility = \", inf_pr)\n    println(\"  Dual infeasibility = \", inf_du)\n    println(\"  Complementarity = \", mu)\n    \n    # Return true to continue, false to stop\n    return iter_count < 5  # Stop after 5 iterations for this example\nend\nnlp = ADNLPModel(x -> (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2, [-1.2; 1.0])\nstats = ipopt(nlp, callback = my_callback, print_level = 0)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"You can also use callbacks with the advanced solver interface:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"# Advanced usage with IpoptSolver\nsolver = IpoptSolver(nlp)\nstats = solve!(solver, nlp, callback = my_callback, print_level = 0)","category":"page"},{"location":"tutorial/#Custom-stopping-criteria","page":"Tutorial","title":"Custom stopping criteria","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Callbacks are particularly useful for implementing custom stopping criteria:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function custom_stopping_callback(alg_mod, iter_count, obj_value, inf_pr, inf_du, mu, d_norm, regularization_size, alpha_du, alpha_pr, ls_trials, args...)\n    # Custom stopping criterion: stop if objective gets close to optimum\n    if obj_value < 0.01\n        println(\"Custom stopping criterion met at iteration $iter_count\")\n        return false  # Stop optimization\n    end\n    \n    return true  # Continue optimization\nend\n\nnlp = ADNLPModel(x -> (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2, [-1.2; 1.0])\nstats = ipopt(nlp, callback = custom_stopping_callback, print_level = 0)","category":"page"},{"location":"tutorial/#Manual-input","page":"Tutorial","title":"Manual input","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In this section, we work through an example where we specify the problem and its derivatives manually. For this, we need to implement the following NLPModel API methods:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"obj(nlp, x): evaluate the objective value at x;\ngrad!(nlp, x, g): evaluate the objective gradient at x;\ncons!(nlp, x, c): evaluate the vector of constraints, if any;\njac_structure!(nlp, rows, cols): fill rows and cols with the spartity structure of the Jacobian, if the problem is constrained;\njac_coord!(nlp, x, vals): fill vals with the Jacobian values corresponding to the sparsity structure returned by jac_structure!();\nhess_structure!(nlp, rows, cols): fill rows and cols with the spartity structure of the lower triangle of the Hessian of the Lagrangian;\nhess_coord!(nlp, x, y, vals; obj_weight=1.0): fill vals with the values of the Hessian of the Lagrangian corresponding to the sparsity structure returned by hess_structure!(), where obj_weight is the weight assigned to the objective, and y is the vector of multipliers.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The model that we implement is a logistic regression model. We consider the model h(beta x) = (1 + e^-beta^Tx)^-1, and the loss function","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"ell(beta) = -sum_i = 1^m y_i ln h(beta x_i) + (1 - y_i) ln(1 - h(beta x_i))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"with regularization lambda beta^2  2.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using DataFrames, LinearAlgebra, NLPModels, NLPModelsIpopt, Random\n\nmutable struct LogisticRegression <: AbstractNLPModel{Float64, Vector{Float64}}\n  X :: Matrix\n  y :: Vector\n  λ :: Real\n  meta :: NLPModelMeta{Float64, Vector{Float64}} # required by AbstractNLPModel\n  counters :: Counters # required by AbstractNLPModel\nend\n\nfunction LogisticRegression(X, y, λ = 0.0)\n  m, n = size(X)\n  meta = NLPModelMeta(n, name=\"LogisticRegression\", nnzh=div(n * (n+1), 2) + n) # nnzh is the length of the coordinates vectors\n  return LogisticRegression(X, y, λ, meta, Counters())\nend\n\nfunction NLPModels.obj(nlp :: LogisticRegression, β::AbstractVector)\n  hβ = 1 ./ (1 .+ exp.(-nlp.X * β))\n  return -sum(nlp.y .* log.(hβ .+ 1e-8) .+ (1 .- nlp.y) .* log.(1 .- hβ .+ 1e-8)) + nlp.λ * dot(β, β) / 2\nend\n\nfunction NLPModels.grad!(nlp :: LogisticRegression, β::AbstractVector, g::AbstractVector)\n  hβ = 1 ./ (1 .+ exp.(-nlp.X * β))\n  g .= nlp.X' * (hβ .- nlp.y) + nlp.λ * β\nend\n\nfunction NLPModels.hess_structure!(nlp :: LogisticRegression, rows :: AbstractVector{<:Integer}, cols :: AbstractVector{<:Integer})\n  n = nlp.meta.nvar\n  I = ((i,j) for i = 1:n, j = 1:n if i ≥ j)\n  rows[1 : nlp.meta.nnzh] .= [getindex.(I, 1); 1:n]\n  cols[1 : nlp.meta.nnzh] .= [getindex.(I, 2); 1:n]\n  return rows, cols\nend\n\nfunction NLPModels.hess_coord!(nlp :: LogisticRegression, β::AbstractVector, vals::AbstractVector; obj_weight=1.0, y=Float64[])\n  n, m = nlp.meta.nvar, length(nlp.y)\n  hβ = 1 ./ (1 .+ exp.(-nlp.X * β))\n  fill!(vals, 0.0)\n  for k = 1:m\n    hk = hβ[k]\n    p = 1\n    for j = 1:n, i = j:n\n      vals[p] += obj_weight * hk * (1 - hk) * nlp.X[k,i] * nlp.X[k,j]\n      p += 1\n    end\n  end\n  vals[nlp.meta.nnzh+1:end] .= nlp.λ * obj_weight\n  return vals\nend\n\nRandom.seed!(0)\n\n# Training set\nm = 1000\ndf = DataFrame(:age => rand(18:60, m), :salary => rand(40:180, m) * 1000)\ndf.buy = (df.age .> 40 .+ randn(m) * 5) .| (df.salary .> 120_000 .+ randn(m) * 10_000)\n\nX = [ones(m) df.age df.age.^2 df.salary df.salary.^2 df.age .* df.salary]\ny = df.buy\n\nλ = 1.0e-2\nnlp = LogisticRegression(X, y, λ)\nstats = ipopt(nlp, print_level=0)\nβ = stats.solution\n\n# Test set - same generation method\nm = 100\ndf = DataFrame(:age => rand(18:60, m), :salary => rand(40:180, m) * 1000)\ndf.buy = (df.age .> 40 .+ randn(m) * 5) .| (df.salary .> 120_000 .+ randn(m) * 10_000)\n\nX = [ones(m) df.age df.age.^2 df.salary df.salary.^2 df.age .* df.salary]\nhβ = 1 ./ (1 .+ exp.(-X * β))\nypred = hβ .> 0.5\n\nacc = count(df.buy .== ypred) / m\nprintln(\"acc = $acc\")","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Plots\ngr()\n\nf(a, b) = dot(β, [1.0; a; a^2; b; b^2; a * b])\nP = findall(df.buy .== true)\nscatter(df.age[P], df.salary[P], c=:blue, m=:square)\nP = findall(df.buy .== false)\nscatter!(df.age[P], df.salary[P], c=:red, m=:xcross, ms=7)\ncontour!(range(18, 60, step=0.1), range(40_000, 180_000, step=1.0), f, levels=[0.5])","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"}]
}
